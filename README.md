**ğŸ“§ DEVELOPMENT OF A PROMPT-DRIVEN EMAIL PRODUCTIVITY AGENT**
**ğŸš€ End-to-end LLM-powered Email Intelligence System (Gemini 2.5 Flash + Python + Streamlit + FastAPI)**

This project implements a full email-processing pipeline using Gemini 2.5 Flash, with:
Email ingestion (categorization + action extraction)
Natural-language powered Email Agent
Draft generation, summarization, Q&A
Persistent conversation memory
Saved draft manager + mock send
Streamlit UI

***âœ… PHASE 1 â€” Email Ingestion Pipeline***
STEP 1 â€” Understanding the assignment
To visualize system architecture, I created diagrams on Mermaid Charts:
Flowchart (Sequence of Operations).png
Component Working Diagram (Architecture + Data Flow).png

STEP 2 â€” Building mock_inbox.json
Created 20 realistic sample emails: Meeting invitations, Newsletter content, Spam-like offers, Requests requiring action and Project updates

STEP 3 â€” Creating prompts.json
A modular prompt file holding all templates:
Categorization, Action extraction, Drafting, Summarization, Intent detection, Q&A agent
This keeps prompts editable without touching code.

STEP 4 â€” Creating llm_client.py (Gemini wrapper)
llm_client.py manages:
Deterministic LLM calls (temperature=0)
Structured output (JSON Schema)
Retry logic
MAX_TOKEN recovery
Diagnostics logging

Install the official Gemini SDK:
pip install google-genai

STEP 5 â€” Creating ingest.py
Responsible for:
Reading mock_inbox.json
Running categorization + action extraction
Producing processed_outputs.json
Fallback recovery (batch â†’ per-email)
Output looks like:
[
  {
    "email_id": "email_001",
    "category": "To-Do",
    "extracted_actions": [...],
    "processed_at": "..."
  }
]

Install deps:
pip install fastapi uvicorn python-dotenv google-genai

Run:
uvicorn app_api:app --reload --port 8000

Visit Swagger UI:
http://localhost:8000/docs


STEP 7 â€” Initial environment setup
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt

.env file:
GEMINI_API_KEY=your_key_here

Test using:
python test_gemini.py


STEP 8 â€” Run ingestion
python ingest.py
This generates processed_outputs.json.

STEP 9 â€” Build Streamlit UI
Install Streamlit:
pip install streamlit pandas

Run:
streamlit run app_streamlit.py

***âœ… PHASE 2 â€” Intelligent Email Agent***
STEP 1 â€” Created agent.py
Capabilities:
Summarization
Natural language Q&A
Task/action re-extraction
Draft generation (tone, length, fallback)
Memory tracking per session
Search emails
List all extracted tasks

All tool functions are modular:
tool_summarize
tool_extract_actions
tool_draft_reply
tool_search_emails
tool_list_actions
Gemini calls are robust, with:
Truncation of long emails
Multiple token-budget retries
Diagnostics logging
Fallback generation

STEP 2 â€” Updated prompts.json
Added templates for:
draft reply
summarization
Q&A agent
rewrite tone
intent detection

STEP 3 â€” Extended Streamlit UI
app_streamlit.py now includes:
Inbox table
Email Agent Chat panel
Memory per session
Safe try/except wrapper around LLM
Conversation history
Download data
Run ingestion button

Memory persisted in:
âœ” memory.json

STEP 4 â€” Test workflow
Run:
streamlit run app_streamlit.py
Use:
Summarize this email â†’ summary
What are the tasks here? â†’ action list
Draft a reply accepting the meeting... â†’ email draft
Find emails about migration â†’ search results
List actions â†’ all tasks

Inspect memory:
memory.json

âœ” What Phase 2 implemented
Intent detection (LLM + heuristic fallback)
Full agent toolset
Robust draft engine (truncation + retries + fallback)
Robust action extractor (structured + fallback text parse)
Working conversation memory
Clean Streamlit UI with error handling
Logging system for failures
Quota/MAX_TOKEN safe design

***âœ… PHASE 3 â€” Drafts Manager + Mock Sending***
STEP 1 â€” Implement draft storage in agent.py
Added:
load_drafts()
save_draft()
mock_send_draft()
list_drafts()

Draft structure in drafts.json:
{
  "id": "draft_1_1732452000",
  "email_id": "email_003",
  "draft_text": "...",
  "created_at": "...",
  "saved_by": "you@company.com",
  "sent": false,
  "sent_at": null
}

Mock send logs:
âœ” logs/mock_sent.log

STEP 2 â€” Updated Streamlit UI
Added:

Save draft button
Mock send button
Saved Drafts Manager
View drafts
Mock-send drafts
Delete drafts

STEP 3 â€” Test full workflow
streamlit run app_streamlit.py

Flow:
Select email
Ask: Draft a reply accepting the meetingâ€¦
Click Save draft â†’ appears in drafts.json
Click Mock send â†’ log entry created
View/delete drafts from Saved drafts panel

ğŸ— Folder Structure
Ocean.AI/
â”‚
â”œâ”€â”€ app_streamlit.py
â”œâ”€â”€ app_api.py
â”œâ”€â”€ agent.py
â”œâ”€â”€ ingest.py
â”œâ”€â”€ llm_client.py
â”‚
â”œâ”€â”€ mock_inbox.json
â”œâ”€â”€ processed_outputs.json
â”œâ”€â”€ prompts.json
â”œâ”€â”€ drafts.json
â”œâ”€â”€ memory.json
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ draft_errors.log
â”‚   â””â”€â”€ mock_sent.log
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ Assignment - 2.pdf


ğŸ”§ Setup Instructions
1. Install dependencies
pip install -r requirements.txt
2. Create .env
GEMINI_API_KEY=YOUR_KEY
3. Run ingestion
python ingest.py
4. Run Streamlit UI
streamlit run app_streamlit.py
5. (Optional) Run FastAPI
uvicorn app_api:app --reload --port 8000


ğŸš€ Demo Script (2â€“3 minutes)
Run:
streamlit run app_streamlit.py
Show inbox table
Select email_003
Ask:
â€œSummarize this emailâ€
â€œWhat are the tasks here?â€
â€œDraft a reply in friendly toneâ€

Click Save Draft
Go to Saved Drafts â†’ show it
Click Mock Send â†’ show log entry
Show memory.json (conversation remembered)
